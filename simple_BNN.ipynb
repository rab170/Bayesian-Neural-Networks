{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD, Adam,Adagrad\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense,Activation\n",
    "\n",
    "from imutils import paths\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import pymc3 as pm\n",
    "\n",
    "from edward.models import Bernoulli, Normal\n",
    "import edward as ed\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def image_to_feature_vector(image, size=(32, 32)): return cv2.resize(image, size).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cats = map(image_to_feature_vector, [cv2.imread(cat) for cat in glob.glob('data/cats/*')])\n",
    "dogs = map(image_to_feature_vector, [cv2.imread(dog) for dog in glob.glob('data/dogs/*')])\n",
    "\n",
    "data = np.vstack((cats,dogs))\n",
    "labels = ['cat']*len(cats) + ['dog']*len(dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "labels = le.fit_transform(labels)\n",
    "data = np.array(data) / 255.0\n",
    "labels = np_utils.to_categorical(labels, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbrusch/Desktop/_notebook/lib/python2.7/site-packages/ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(768, activation=\"tanh\", kernel_initializer=\"uniform\", input_dim=3072)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/kbrusch/Desktop/_notebook/lib/python2.7/site-packages/ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(384, activation=\"tanh\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(768, input_dim=3072, init=\"uniform\",\n",
    "\tactivation=\"tanh\"))\n",
    "model.add(Dense(384, init=\"uniform\", activation=\"tanh\"))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Epoch 1/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 2/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 3/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 4/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 5/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 6/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 7/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 8/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 9/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 10/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 11/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 12/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 13/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 14/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 15/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 16/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 17/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 18/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 19/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 20/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 21/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 22/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 23/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 24/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 25/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 26/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 27/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 28/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 29/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 30/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 31/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 32/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 33/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 34/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 35/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 36/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 37/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 38/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 39/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 40/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 41/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 42/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 43/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 44/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 45/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 46/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 47/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 48/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 49/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 50/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 51/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 52/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 53/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 54/100\n",
      "300/300 [==============================] - ETA: 0s - loss: 7.7646 - acc: 0.515 - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 55/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 56/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 57/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 58/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 59/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 60/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 61/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 62/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 63/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 64/100\n",
      "300/300 [==============================] - ETA: 0s - loss: 7.5768 - acc: 0.527 - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 65/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 66/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 67/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 68/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 69/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 70/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 71/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 72/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 73/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 74/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 75/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 76/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 77/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 78/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 79/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 80/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 81/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 82/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 83/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 84/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 85/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 86/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 87/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 88/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 89/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 90/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 91/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 92/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 93/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 94/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 95/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 96/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 97/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 98/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 99/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n",
      "Epoch 100/100\n",
      "300/300 [==============================] - 0s - loss: 7.9083 - acc: 0.5067     \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13bdb7950>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "print(\"[INFO] compiling model...\")\n",
    "sgd = SGD(lr=0.01)\n",
    "adagrad = Adagrad(lr=0.01)\n",
    "adam = Adam(lr=0.01)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])\n",
    "model.fit(trainData, trainLabels, nb_epoch=100, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating on testing set...\n",
      "100/100 [==============================] - 0s\n",
      "[INFO] loss=8.3357, accuracy: 48.0000%\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating on testing set...\")\n",
    "(loss, accuracy) = model.evaluate(testData, testLabels, batch_size=128, verbose=1)\n",
    "print(\"[INFO] loss={:.4f}, accuracy: {:.4f}%\".format(loss, accuracy*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 3072)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kbrusch/Desktop/_notebook/lib/python2.7/site-packages/ipykernel_launcher.py:13: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(768, activation=\"relu\", kernel_initializer=\"uniform\", input_dim=3072)`\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Normal' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-d668e6fbaebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mw_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m768\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3072\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mz_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mw_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m384\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"uniform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Normal' object is not callable"
     ]
    }
   ],
   "source": [
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "from edward.models import Bernoulli, Normal\n",
    "import edward as ed\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "N = 400  # number of data points, 400 if straight from 'data'\n",
    "D = 3072   # number of features\n",
    "\n",
    "\n",
    "z_0 = Normal(loc=tf.zeros([N, D]), scale=tf.ones([N, D]))\n",
    "w_0 = Dense(768, input_dim=3072, init=\"uniform\",activation=\"relu\")(z_0)\n",
    "\n",
    "z_1 = Normal(loc=tf.zeros([N, D]), scale=tf.ones([N, D]))(w_0)\n",
    "w_1 = Dense(384, init=\"uniform\", activation=\"relu\")(z_1)\n",
    "\n",
    "#z_2 = \n",
    "#w_2 = Dense(2, activation=(\"softmax\"))\n",
    "\n",
    "\n",
    "#Normal()\n",
    "\n",
    "#X = tf.placeholder(tf.float32, [N, D])\n",
    "#y = Bernoulli(logits=neural_network(X))\n",
    "\n",
    "#inference = ed.KLqp({z: qz}, data={x: x_ph})\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.01, epsilon=1.0)\n",
    "#inference.initialize(optimizer=optimizer)\n",
    "\n",
    "#sess = ed.get_session()\n",
    "#init = tf.global_variables_initializer()\n",
    "#init.run()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 22s | Loss: 376.572\n"
     ]
    }
   ],
   "source": [
    "(trainData, testData, trainLabels, testLabels) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "def neural_network(X):\n",
    "  h = tf.tanh(tf.matmul(X, W_0) + b_0)\n",
    "  h = tf.tanh(tf.matmul(h, W_1) + b_1)\n",
    "  h = tf.matmul(h, W_2) + b_2\n",
    "  return tf.reshape(h, [-1])\n",
    "\n",
    "N = 400  # number of data points, 400 if straight from 'data'\n",
    "D = 3072   # number of features\n",
    "\n",
    "\n",
    "# MODEL\n",
    "W_0 = Normal(loc=tf.zeros([D, 10]), scale=tf.ones([D, 10]))\n",
    "W_1 = Normal(loc=tf.zeros([10, 10]), scale=tf.ones([10, 10]))\n",
    "W_2 = Normal(loc=tf.zeros([10, 1]), scale=tf.ones([10, 1]))\n",
    "b_0 = Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
    "b_1 = Normal(loc=tf.zeros(10), scale=tf.ones(10))\n",
    "b_2 = Normal(loc=tf.zeros(1), scale=tf.ones(1))\n",
    "\n",
    "X = tf.placeholder(tf.float32, [N, D])\n",
    "y = Bernoulli(logits=neural_network(X))\n",
    "\n",
    "# INFERENCE\n",
    "qW_0 = Normal(loc=tf.Variable(tf.random_normal([D, 10])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([D, 10]))))\n",
    "qW_1 = Normal(loc=tf.Variable(tf.random_normal([10, 10])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([10, 10]))))\n",
    "qW_2 = Normal(loc=tf.Variable(tf.random_normal([10, 1])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([10, 1]))))\n",
    "qb_0 = Normal(loc=tf.Variable(tf.random_normal([10])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([10]))))\n",
    "qb_1 = Normal(loc=tf.Variable(tf.random_normal([10])),\n",
    "              scale=tf.nn.softplus(tf.Variable(tf.random_normal([10]))))\n",
    "qb_2 = Normal(loc=tf.Variable(tf.random_normal([1])),\n",
    "              scale=tf.nn.softmax(tf.Variable(tf.random_normal([1]))))\n",
    "\n",
    "\n",
    "\n",
    "#type(data)\n",
    "labels = [1]*len(cats) + [0]*len(dogs)\n",
    "np.array(labels).shape\n",
    "inference = ed.KLqp({W_0: qW_0, b_0: qb_0,\n",
    "                     W_1: qW_1, b_1: qb_1,\n",
    "                     W_2: qW_2, b_2: qb_2}, data={X: data, y: labels})\n",
    "#inference = ed.MonteCarlo({W_0: qW_0, b_0: qb_0,\n",
    "#                     W_1: qW_1, b_1: qb_1,\n",
    "#                     W_2: qW_2, b_2: qb_2}, data={X: data, y: labels})\n",
    "\n",
    "\n",
    "inference.run()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'zeros_3:0' shape=(100, 10) dtype=float32>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.zeros([100, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theano' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3f818c0a6f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mann_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mann_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'theano' is not defined"
     ]
    }
   ],
   "source": [
    "X, Y = data, labels\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)\n",
    "ann_input = theano.shared(data)\n",
    "ann_output = theano.shared(labels)\n",
    "\n",
    "# Initialize random weights between each layer\n",
    "init_1 = np.random.randn(X.shape[1], 768)\n",
    "init_2 = np.random.randn(768, 384)\n",
    "init_out = np.random.randn(384)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/theano/tensor/basic.py:2146: UserWarning: theano.tensor.round() changed its default from `half_away_from_zero` to `half_to_even` to have the same default as NumPy. Use the Theano flag `warn.round=False` to disable this warning.\n",
      "  \"theano.tensor.round() changed its default from\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[0].shape[1] = 2, input[1].shape[1] = 400)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-463dc0dd1b5b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                        \u001b[0mact_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                        observed=ann_output)\n\u001b[0m\u001b[1;32m     30\u001b[0m \"\"\"\n\u001b[1;32m     31\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pymc3/distributions/distribution.pyc\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'observed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Name needs to be a string but got: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pymc3/model.pyc\u001b[0m in \u001b[0;36mVar\u001b[0;34m(self, name, dist, data)\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             var = ObservedRV(name=name, data=data,\n\u001b[0;32m--> 303\u001b[0;31m                              distribution=dist, model=self)\n\u001b[0m\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobserved_RVs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pymc3/model.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, type, owner, index, name, data, distribution, model)\u001b[0m\n\u001b[1;32m    584\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissing_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogp_elemwiset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/pymc3/distributions/discrete.pyc\u001b[0m in \u001b[0;36mlogp\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         return bound(\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             p >= 0, p <= 1)\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mthunk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mstorage_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m                 \u001b[0mrequired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrequired\u001b[0m  \u001b[0;31m# We provided all inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[0;34m()\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 843\u001b[0;31m             \u001b[0mfill_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    844\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/theano/gof/cc.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1696\u001b[0m                 \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_storage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1698\u001b[0;31m             \u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1699\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/six.pyc\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension mis-match. (input[0].shape[1] = 2, input[1].shape[1] = 400)"
     ]
    }
   ],
   "source": [
    "with pm.Model() as neural_network:\n",
    "    # Weights from input to hidden layer\n",
    "    weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n",
    "                             shape=(X.shape[1], 768), \n",
    "                             testval=init_1)\n",
    "    \n",
    "    # Weights from 1st to 2nd layer\n",
    "    weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n",
    "                            shape=(768, 384), \n",
    "                            testval=init_2)\n",
    "    \n",
    "    # Weights from hidden lay2er to output\n",
    "    weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n",
    "                              shape=(384,), \n",
    "                              testval=init_out)\n",
    "    \n",
    "    # Build neural-network using tanh activation function\n",
    "    act_1 = T.nnet.relu(T.dot(ann_input, \n",
    "                         weights_in_1))\n",
    "    act_2 = T.nnet.relu(T.dot(act_1, \n",
    "                         weights_1_2))\n",
    "    act_out = T.nnet.sigmoid(T.dot(act_2, \n",
    "                                   weights_2_out))\n",
    "    \n",
    "    # Binary classification -> Bernoulli likelihood\n",
    "    out = pm.Bernoulli('out', \n",
    "                       act_out,\n",
    "\n",
    "                       observed=ann_output)\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "model.add(Dense(768, input_dim=3072, init=\"uniform\",\n",
    "\tactivation=\"relu\"))\n",
    "model.add(Dense(384, init=\"uniform\", activation=\"relu\"))\n",
    "model.add(Dense(2))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
